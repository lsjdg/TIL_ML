{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Word2Vec"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/leeseungjun/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","from lxml import etree\n","import urllib.request\n","import zipfile\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["('ted_en-20160408.xml', <http.client.HTTPMessage at 0x169a2ead0>)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["urllib.request.urlretrieve(\n","    \"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\n","    filename=\"ted_en-20160408.xml\",\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["targetXML = open(\"ted_en-20160408.xml\", \"r\", encoding=\"UTF8\")\n","target_text = etree.parse(targetXML)\n","\n","# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n","parse_text = \"\\n\".join(target_text.xpath(\"//content/text()\"))\n","\n","# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n","# 해당 코드는 괄호로 구성된 내용을 제거.\n","content_text = re.sub(r\"\\([^)]*\\)\", \"\", parse_text)"]},{"cell_type":"markdown","metadata":{},"source":["### 데이터 살펴보기"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo m\""]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["content_text[:100]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# 입력 코퍼스에 대해서 NLTK를 이용해 문장 토큰화 수행\n","sent_text = sent_tokenize(content_text)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["[\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\",\n"," 'To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation.',\n"," 'Both are necessary, but it can be too much of a good thing.']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["sent_text[:3]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n","normalized_text = []\n","for string in sent_text:\n","    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n","    normalized_text.append(tokens)\n","\n","# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n","result = [word_tokenize(sentence) for sentence in normalized_text]"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']]\n"]}],"source":["print(result[:3])"]},{"cell_type":"markdown","metadata":{},"source":["### 영어 word2vec 훈련"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","model = Word2Vec(\n","    sentences=result,  # corpus\n","    vector_size=100,  # 각 단어의 임베딩된 차원 => 단어 벡터의 크기\n","    window=5,  # context window 의 크기\n","    min_count=5,  # 단어 최소 빈도수 제한\n","    workers=4,  # 학습을 위한 cpu 갯수\n","    sg=1,  # skip gram 사용 여부 => 0 : CBOW , 1 : skip gram\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["[('woman', 0.7464737296104431),\n"," ('guy', 0.7192587852478027),\n"," ('soldier', 0.6922350525856018),\n"," ('rabbi', 0.6873827576637268),\n"," ('boy', 0.6782869100570679),\n"," ('son', 0.6768252849578857),\n"," ('adage', 0.6663954257965088),\n"," ('michelangelo', 0.6655485033988953),\n"," ('handsome', 0.6485349535942078),\n"," ('shepherd', 0.6447014808654785)]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# 단어에 대한 유사도 구하기\n","# most_simalar : cosine similarity\n","model.wv.most_similar(\"man\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["[('girl', 0.7980387210845947),\n"," ('man', 0.7464736700057983),\n"," ('child', 0.743230938911438),\n"," ('lady', 0.7197898626327515),\n"," ('soldier', 0.7186397314071655),\n"," ('boy', 0.7042767405509949),\n"," ('parent', 0.7007882595062256),\n"," ('husband', 0.6846175789833069),\n"," ('son', 0.677384614944458),\n"," ('daughter', 0.6771542429924011)]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model.wv.most_similar(\"woman\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["array([-0.25788373, -0.44084692, -0.01848858, -0.41008654,  0.06461044,\n","       -0.00098784,  0.04530404,  0.27290663, -0.24753846,  0.04860221,\n","       -0.23623542, -0.40986082,  0.20925735,  0.2960284 , -0.33408472,\n","       -0.1963368 , -0.2462806 , -0.02860613,  0.05724161, -0.39650035,\n","        0.4787234 ,  0.0252343 ,  0.47673836, -0.04523695,  0.15992147,\n","       -0.22721612, -0.27062297, -0.22787645, -0.29539725, -0.18134838,\n","        0.2610867 ,  0.40853724,  0.22275853,  0.15678097,  0.00819362,\n","        0.25696972, -0.16722701,  0.00358507, -0.11458305, -0.03936708,\n","        0.02665408, -0.3851479 , -0.17574129, -0.06721475, -0.07976396,\n","       -0.38861525, -0.2516326 ,  0.07662547, -0.22281861,  0.08232789,\n","       -0.06064706, -0.46023953,  0.02285396,  0.36257708,  0.10212065,\n","       -0.0214293 , -0.15527436, -0.6803391 , -0.43982115, -0.3863792 ,\n","       -0.35221553,  0.25182706, -0.0345569 , -0.12098335, -0.5997773 ,\n","       -0.06665862, -0.21589656,  0.3067192 , -0.28151193,  0.28513837,\n","       -0.16125348,  0.09603105,  0.49028558,  0.03407502, -0.03321206,\n","       -0.06985684,  0.04040944,  0.1640464 , -0.56399703,  0.00852261,\n","       -0.03723766, -0.04757516, -0.67223084,  0.5181765 , -0.42901102,\n","       -0.26888126,  0.3917124 ,  0.21137415, -0.2190038 , -0.5000025 ,\n","       -0.13656417,  0.34031194,  0.00666194,  0.22424075,  0.6010358 ,\n","        0.21473822,  0.2982676 , -0.12322796, -0.47632542,  0.278301  ],\n","      dtype=float32)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# 모델이 학습한 벡터를 확인\n","model.wv[\"man\"]  # wv는 딕셔너리"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["100"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["len(model.wv[\"man\"])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# 저장\n","from gensim.models import KeyedVectors\n","\n","model.wv.save_word2vec_format(\"../../data/nlp/w2v/eng_w2v\")\n","\n","loaded_model = KeyedVectors.load_word2vec_format(\"../../data/nlp/w2v/eng_w2v\")"]},{"cell_type":"markdown","metadata":{},"source":["## FastText"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["[('cognition', 0.6716188192367554),\n"," ('implanted', 0.6652306318283081),\n"," ('plasticity', 0.6529497504234314),\n"," ('locomotion', 0.6473685503005981),\n"," ('cpu', 0.6427909731864929),\n"," ('brain', 0.6423183679580688),\n"," ('appetite', 0.6408774852752686),\n"," ('willpower', 0.640343189239502),\n"," ('wiring', 0.6349008679389954),\n"," ('simulator', 0.633720338344574)]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["loaded_model.most_similar(\"memory\")"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"\"Key 'memorrry' not present in vocabulary\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemorrry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 오타가 난다면?\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# w2v 는 dict 기반이기 때문에 없는 단어가 나오면 오류 발생\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/envs/dl-env/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n","File \u001b[0;32m/opt/anaconda3/envs/dl-env/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n","\u001b[0;31mKeyError\u001b[0m: \"Key 'memorrry' not present in vocabulary\""]}],"source":["loaded_model.most_similar(\"memorrry\")  # 오타가 난다면?\n","# w2v 는 dict 기반이기 때문에 없는 단어가 나오면 오류 발생"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["from gensim.models import FastText\n","\n","fasttext_model = FastText(\n","    sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=1\n",")"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["[('memo', 0.8510879874229431),\n"," ('memoir', 0.8043996691703796),\n"," ('forgery', 0.7965269684791565),\n"," ('memorize', 0.7776286602020264),\n"," ('memory', 0.7707096934318542),\n"," ('nemo', 0.7623752355575562),\n"," ('memoirs', 0.7604741454124451),\n"," ('emory', 0.757304310798645),\n"," ('rehearsal', 0.7433197498321533),\n"," ('tattoo', 0.7401583194732666)]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["fasttext_model.wv.most_similar(\"memorrry\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
