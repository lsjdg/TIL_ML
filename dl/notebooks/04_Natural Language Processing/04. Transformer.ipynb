{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "train_model = False  # 실제 훈련을 할 시 True 로 변경\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# capture는 주렁주렁 출력 보여주지 않게끔\n",
    "# Font 설치\n",
    "!brew tap homebrew/cask-fonts\n",
    "!brew install --cask font-nanum\n",
    "!fc-cache -fv\n",
    "!rm -rf ~/Library/Caches/matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"font\", family=\"NanumBarunGothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현\n",
    "\n",
    "* encoder, decoder 의 self attention 을 수행하는 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### einops\n",
    "* 장점 : 텐서 변환을 매우 쉽게 이해하고 변환\n",
    "* 단점 : 디버깅 힘들고 속도 느리다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "# 차원 재배치\n",
    "x = torch.randn(2, 3, 4)\n",
    "y = rearrange(x, \"a b c -> c a b\")  # 차원의 이름을 임의로 지정할 수 있다\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 차원 나누기\n",
    "x = torch.randn(6, 4)\n",
    "y = rearrange(\n",
    "    x, \"(a b) c -> a b c\", a=2, b=3\n",
    ")  # x 의 첫 번쨰 차원이 a 와 b 로 이루어져 있다\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention (MHA) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        d_model : dimension of embedding vector\n",
    "        n_heads : num of heads in MHA\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Q, K, V 벡터를 위한 linear layer\n",
    "        self.fc_q = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.fc_k = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.fc_v = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "\n",
    "        # 최종 출력을 위한 linear layer\n",
    "        self.fc_o = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "\n",
    "        # scaling for attention score\n",
    "        self.scale = torch.sqrt(\n",
    "            torch.tensor(d_model / n_heads)\n",
    "        )  # GPU 에서의 연산을 위해 tensor 로 변환\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V : embedding vector of each word, (batch_size, max_len, dim)\n",
    "        mask : mask for attention score (optional) => init as None\n",
    "        \"\"\"\n",
    "\n",
    "        # linear transformation on embedding vector\n",
    "        Q = self.fc_q(Q)\n",
    "        K = self.fc_k(K)\n",
    "        V = self.fc_v(V)\n",
    "\n",
    "        # MHA 를 위해 embedding dim 을 n_head 개로 분할\n",
    "        # ex. (32, 128, 512) -> (32, 8, 128, 64)\n",
    "        Q = rearrange(Q, \"N t (h dk) -> N h t dk\", h=self.n_heads)\n",
    "        K = rearrange(K, \"N t (h dk) -> N h t dk\", h=self.n_heads)\n",
    "        V = rearrange(V, \"N t (h dk) -> N h t dk\", h=self.n_heads)\n",
    "\n",
    "        # attention score before softmax\n",
    "        attention_score = (\n",
    "            Q @ K.transpose(-2, 1) / self.scale\n",
    "        )  # (N, h, t, d_k) @ (N, h, d_k, t) -> (N, h, t(Query 길이), t(Key 길이))\n",
    "\n",
    "        # masking on padding\n",
    "        if mask is not None:\n",
    "            attention_score[mask] = -1e10\n",
    "\n",
    "        # softmax on Key\n",
    "        energy = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        attention = energy @ V  # (N, h, t, t) @ (N, h, t, dk) -> (N, h, t, dk)\n",
    "\n",
    "        # head 차원을 연결해서 원래의 차원으로 되돌리기\n",
    "        x = rearrange(attention, \"N h t dk -> N t (h dk)\")  # (N, h, t, dk) -> (N, t, D)\n",
    "\n",
    "        # 최종 출력값에 대해 linear transformation (모든 head 의 분석 결과를 종합)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network (FNN)\n",
    "* encoder, decoder 의 MHA 결과를 하나로 합쳐준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model=512, d_ff=2048, drop_p=0.1\n",
    "    ):  # dropout : 차원 확장에 의한 overfitting 방지\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, mha_output):\n",
    "        out = self.linear(mha_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA - FFN 연결 과정, skip connection, LN\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
    "        \"\"\"\n",
    "        d_model : 임베딩 벡터 차원\n",
    "        d_ff : feed forward 은닉층 차원\n",
    "        n_heads : MHA head 갯수\n",
    "        drop_p : dropout 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # MHA (Self Attention)\n",
    "        self.self_atten = MHA(d_model=d_model, n_heads=n_heads)\n",
    "\n",
    "        # MHA 에 대한 layer normalization\n",
    "        self.self_atten_LN = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "        # feed forward network\n",
    "        self.FF = FeedForward(d_model=d_model, d_ff=d_ff, drop_p=drop_p)\n",
    "\n",
    "        # feed forward 출력에 대한 LN\n",
    "        self.FF_LN = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x, enc_mask):\n",
    "        \"\"\"\n",
    "        x : input tensor (batch_size, seq_len, d_model)\n",
    "        enc_mask : input mask (batch_size, 1, seq_len)\n",
    "\n",
    "        return : encoder output, energy\n",
    "        \"\"\"\n",
    "\n",
    "        # MHA block\n",
    "        residual, atten_enc = self.self_atten(Q=x, K=x, V=x, mask=enc_mask)\n",
    "        residual = self.dropout(residual)\n",
    "\n",
    "        # Skip Connection & LN\n",
    "        encoder_self_attention_output = self.self_atten_LN(x + residual)\n",
    "\n",
    "        # FFN block\n",
    "        residual = self.FF(encoder_self_attention_output)\n",
    "        residual = self.dropout(residual)\n",
    "\n",
    "        # Skip Connection & LN\n",
    "        encoder_ffn_output = self.FF_LN(encoder_self_attention_output + residual)\n",
    "\n",
    "        return encoder_ffn_output, atten_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_embedding : 입력 임베딩 레이어 (nn.Embedding)\n",
    "        max_len : input sequence 의 최대 길이 (int)\n",
    "        n_layers : encoder layer 의 개수\n",
    "        d_model : embedding vector 의 차원\n",
    "        d_ff : feed forward의 은닉층의 차원\n",
    "        n_heads : MHA의 head 개수\n",
    "        drop_p : dropout 비율\n",
    "        \"\"\"\n",
    "\n",
    "        # d_model 의 제곱근 값으로 scale -> embedding vector 크기 조정\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model))\n",
    "\n",
    "        # input embedding layer\n",
    "        self.input_embedding = input_embedding\n",
    "\n",
    "        # 위치 embedding layer : 위치 정보를 학습하기 위한 layer\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        # 여러 개의 encoder layer 를 쌓기 위해 ModuleList 활용\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, mask, atten_map_save=False):\n",
    "        \"\"\"\n",
    "        src : input sequence (batch_size, seq_len)\n",
    "        mask : mask for padding (batch_size, 1, seq_len)\n",
    "        atten_map_save : attention map 을 저장할지 여부\n",
    "        \"\"\"\n",
    "        # 위치 index tensor 생성 : 각 batch 에서 sequence 길이만큼 위치 index 반복\n",
    "        pos = torch.arange(src.shape[1]).repeat(src.shape[0], 1).to(self.device)\n",
    "\n",
    "        # input embedding 과 위치 embedding 을 합해 input tensor 생성\n",
    "        x_embedding = self.input_embedding(src) + self.pos_embedding(pos)\n",
    "\n",
    "        x_embedding = self.dropout(x_embedding)\n",
    "\n",
    "        # 첫 입력\n",
    "        encoder_output = x_embedding\n",
    "\n",
    "        # energy 를 저장할 텐서\n",
    "        atten_encs = torch.tensor([]).to(self.device)\n",
    "\n",
    "        # 각 encoder layer 를 순차적으로 통과\n",
    "        for layer in self.layers:\n",
    "            encoder_output, atten_enc = layer(encoder_output, mask)\n",
    "\n",
    "            if atten_map_save:\n",
    "                atten_encs = torch.cat([atten_encs, atten_enc[0].unsqueeze()])\n",
    "\n",
    "        return encoder_output, atten_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
