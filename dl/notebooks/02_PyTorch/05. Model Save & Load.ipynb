{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FashionMNIST DataSet & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"../../data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"../../data\", train=False, download=True, transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module 생성\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # 레이어 정의\n",
    "        self.flatten = nn.Flatten()  # nn 내의 Flatten 클래스 이용\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        y = self.linear_relu_stack(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # 이미 여기에 소프트맥스 함수가 포함되어 있다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # DataLoader 에 들어있는 tensor 들을 모델과 같은 위치에 두기\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()  # 이전 배치의 기울기 제거\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 배치가 100번 돌 때마다 화면에 출력\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Train Loss : {loss:>7f} [ {current:>5d} / {size:>5d} ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # loss는 배치 별로 계산, correct는 전체 데이터 세트에 대한 평균 정확도\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # 모델을 추론 모드로 바꿔준다.\n",
    "    model.eval()\n",
    "\n",
    "    # 추론 과정에서는 기울기를 구할 필요 없음\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # 10개의 예측값 중 가장 큰 곳의 인덱스를 argmax 로 찾고, target 과 일치하는지 확인\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(\n",
    "        f\"Test Error : \\n Accuracy : {(100*correct):>0.1f}%, Avg Loss : {test_loss:>8f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "........................\n",
      "Train Loss : 2.294576 [     0 / 60000 ]\n",
      "Train Loss : 1.472597 [  6400 / 60000 ]\n",
      "Train Loss : 0.927392 [ 12800 / 60000 ]\n",
      "Train Loss : 0.917044 [ 19200 / 60000 ]\n",
      "Train Loss : 0.829486 [ 25600 / 60000 ]\n",
      "Train Loss : 0.672047 [ 32000 / 60000 ]\n",
      "Train Loss : 0.812730 [ 38400 / 60000 ]\n",
      "Train Loss : 0.700812 [ 44800 / 60000 ]\n",
      "Train Loss : 0.572073 [ 51200 / 60000 ]\n",
      "Train Loss : 0.630962 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 78.7%, Avg Loss : 0.626356\n",
      "\n",
      "Epoch 2\n",
      "........................\n",
      "Train Loss : 0.704034 [     0 / 60000 ]\n",
      "Train Loss : 0.433490 [  6400 / 60000 ]\n",
      "Train Loss : 0.640886 [ 12800 / 60000 ]\n",
      "Train Loss : 0.389475 [ 19200 / 60000 ]\n",
      "Train Loss : 0.587698 [ 25600 / 60000 ]\n",
      "Train Loss : 0.571984 [ 32000 / 60000 ]\n",
      "Train Loss : 0.630846 [ 38400 / 60000 ]\n",
      "Train Loss : 0.800338 [ 44800 / 60000 ]\n",
      "Train Loss : 0.466468 [ 51200 / 60000 ]\n",
      "Train Loss : 0.474491 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 81.8%, Avg Loss : 0.534396\n",
      "\n",
      "Epoch 3\n",
      "........................\n",
      "Train Loss : 0.590883 [     0 / 60000 ]\n",
      "Train Loss : 0.488239 [  6400 / 60000 ]\n",
      "Train Loss : 0.405648 [ 12800 / 60000 ]\n",
      "Train Loss : 0.568897 [ 19200 / 60000 ]\n",
      "Train Loss : 0.458652 [ 25600 / 60000 ]\n",
      "Train Loss : 0.466411 [ 32000 / 60000 ]\n",
      "Train Loss : 0.415619 [ 38400 / 60000 ]\n",
      "Train Loss : 0.359743 [ 44800 / 60000 ]\n",
      "Train Loss : 0.422191 [ 51200 / 60000 ]\n",
      "Train Loss : 0.468654 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 82.8%, Avg Loss : 0.500054\n",
      "\n",
      "Epoch 4\n",
      "........................\n",
      "Train Loss : 0.354839 [     0 / 60000 ]\n",
      "Train Loss : 0.446321 [  6400 / 60000 ]\n",
      "Train Loss : 0.505832 [ 12800 / 60000 ]\n",
      "Train Loss : 0.521122 [ 19200 / 60000 ]\n",
      "Train Loss : 0.452642 [ 25600 / 60000 ]\n",
      "Train Loss : 0.402973 [ 32000 / 60000 ]\n",
      "Train Loss : 0.638439 [ 38400 / 60000 ]\n",
      "Train Loss : 0.516122 [ 44800 / 60000 ]\n",
      "Train Loss : 0.409123 [ 51200 / 60000 ]\n",
      "Train Loss : 0.405606 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.2%, Avg Loss : 0.479586\n",
      "\n",
      "Epoch 5\n",
      "........................\n",
      "Train Loss : 0.483152 [     0 / 60000 ]\n",
      "Train Loss : 0.378135 [  6400 / 60000 ]\n",
      "Train Loss : 0.504459 [ 12800 / 60000 ]\n",
      "Train Loss : 0.469750 [ 19200 / 60000 ]\n",
      "Train Loss : 0.481913 [ 25600 / 60000 ]\n",
      "Train Loss : 0.357943 [ 32000 / 60000 ]\n",
      "Train Loss : 0.517711 [ 38400 / 60000 ]\n",
      "Train Loss : 0.500232 [ 44800 / 60000 ]\n",
      "Train Loss : 0.469370 [ 51200 / 60000 ]\n",
      "Train Loss : 0.341506 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.8%, Avg Loss : 0.466570\n",
      "\n",
      "Epoch 6\n",
      "........................\n",
      "Train Loss : 0.550200 [     0 / 60000 ]\n",
      "Train Loss : 0.369335 [  6400 / 60000 ]\n",
      "Train Loss : 0.527325 [ 12800 / 60000 ]\n",
      "Train Loss : 0.512707 [ 19200 / 60000 ]\n",
      "Train Loss : 0.377177 [ 25600 / 60000 ]\n",
      "Train Loss : 0.354806 [ 32000 / 60000 ]\n",
      "Train Loss : 0.618965 [ 38400 / 60000 ]\n",
      "Train Loss : 0.320618 [ 44800 / 60000 ]\n",
      "Train Loss : 0.405941 [ 51200 / 60000 ]\n",
      "Train Loss : 0.446935 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.9%, Avg Loss : 0.455450\n",
      "\n",
      "Epoch 7\n",
      "........................\n",
      "Train Loss : 0.372620 [     0 / 60000 ]\n",
      "Train Loss : 0.642119 [  6400 / 60000 ]\n",
      "Train Loss : 0.295777 [ 12800 / 60000 ]\n",
      "Train Loss : 0.347981 [ 19200 / 60000 ]\n",
      "Train Loss : 0.327217 [ 25600 / 60000 ]\n",
      "Train Loss : 0.377162 [ 32000 / 60000 ]\n",
      "Train Loss : 0.337563 [ 38400 / 60000 ]\n",
      "Train Loss : 0.388608 [ 44800 / 60000 ]\n",
      "Train Loss : 0.390516 [ 51200 / 60000 ]\n",
      "Train Loss : 0.484758 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 84.3%, Avg Loss : 0.447549\n",
      "\n",
      "Epoch 8\n",
      "........................\n",
      "Train Loss : 0.431744 [     0 / 60000 ]\n",
      "Train Loss : 0.319299 [  6400 / 60000 ]\n",
      "Train Loss : 0.438794 [ 12800 / 60000 ]\n",
      "Train Loss : 0.386719 [ 19200 / 60000 ]\n",
      "Train Loss : 0.317810 [ 25600 / 60000 ]\n",
      "Train Loss : 0.369044 [ 32000 / 60000 ]\n",
      "Train Loss : 0.289374 [ 38400 / 60000 ]\n",
      "Train Loss : 0.584863 [ 44800 / 60000 ]\n",
      "Train Loss : 0.364945 [ 51200 / 60000 ]\n",
      "Train Loss : 0.331839 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 84.7%, Avg Loss : 0.433213\n",
      "\n",
      "Epoch 9\n",
      "........................\n",
      "Train Loss : 0.416873 [     0 / 60000 ]\n",
      "Train Loss : 0.258737 [  6400 / 60000 ]\n",
      "Train Loss : 0.456011 [ 12800 / 60000 ]\n",
      "Train Loss : 0.230553 [ 19200 / 60000 ]\n",
      "Train Loss : 0.388656 [ 25600 / 60000 ]\n",
      "Train Loss : 0.562224 [ 32000 / 60000 ]\n",
      "Train Loss : 0.639164 [ 38400 / 60000 ]\n",
      "Train Loss : 0.543922 [ 44800 / 60000 ]\n",
      "Train Loss : 0.428595 [ 51200 / 60000 ]\n",
      "Train Loss : 0.275822 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 84.8%, Avg Loss : 0.427074\n",
      "\n",
      "Epoch 10\n",
      "........................\n",
      "Train Loss : 0.535744 [     0 / 60000 ]\n",
      "Train Loss : 0.339659 [  6400 / 60000 ]\n",
      "Train Loss : 0.316621 [ 12800 / 60000 ]\n",
      "Train Loss : 0.366372 [ 19200 / 60000 ]\n",
      "Train Loss : 0.280535 [ 25600 / 60000 ]\n",
      "Train Loss : 0.292071 [ 32000 / 60000 ]\n",
      "Train Loss : 0.434583 [ 38400 / 60000 ]\n",
      "Train Loss : 0.277755 [ 44800 / 60000 ]\n",
      "Train Loss : 0.501861 [ 51200 / 60000 ]\n",
      "Train Loss : 0.556227 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 85.0%, Avg Loss : 0.423008\n",
      "\n",
      "Experiment Successful\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}\\n........................\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Experiment Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련된 모델의 가중치를 저장 / 불러오기\n",
    "\n",
    "불러올 곳에서 모델의 구조를 알고 있는 경우 가중치만 저장하면 적은 용량으로 저장하고 불러오는 것이 가능하다\n",
    "* 단점 : 모델 class 가 없으면 사용할 수 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict() : 모델 내에 있는 레이어 별 가중치를 들고 있는 딕셔너리\n",
    "torch.save(model.state_dict(), \"../../data/saved_models/model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0250,  0.0063, -0.0424,  ..., -0.0664, -0.0115,  0.0079],\n",
       "                      [ 0.0514,  0.0003,  0.0490,  ...,  0.0146,  0.0017, -0.0069],\n",
       "                      [-0.0597,  0.0021, -0.0734,  ...,  0.0013,  0.0351,  0.0053],\n",
       "                      ...,\n",
       "                      [ 0.0384,  0.0221, -0.0080,  ..., -0.0012,  0.0494,  0.0481],\n",
       "                      [ 0.0424,  0.0124,  0.0047,  ...,  0.0114, -0.0058, -0.0607],\n",
       "                      [ 0.0575, -0.0579,  0.0144,  ..., -0.0789, -0.0832, -0.0619]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-0.1647,  0.1012, -0.0167, -0.2256,  0.1592,  0.0787, -0.0628,  0.2006,\n",
       "                      -0.0430,  0.0539,  0.0242, -0.0885,  0.1619,  0.1670, -0.0418,  0.1252,\n",
       "                       0.0539,  0.1613,  0.1228,  0.1240, -0.1476,  0.0523,  0.0990, -0.0581,\n",
       "                       0.1194,  0.2538,  0.0134, -0.0662,  0.0230,  0.0564,  0.2213, -0.0955,\n",
       "                      -0.0445,  0.0357,  0.0270, -0.0885, -0.1651,  0.1403,  0.0884,  0.1271,\n",
       "                       0.0807, -0.1956, -0.1492, -0.1486, -0.0199,  0.1474,  0.2443,  0.0087,\n",
       "                       0.0270,  0.1385,  0.0716, -0.0953,  0.2013,  0.0183, -0.0858,  0.0673,\n",
       "                      -0.0185, -0.0887, -0.0658,  0.1626,  0.2828, -0.0784,  0.0935, -0.0738,\n",
       "                       0.0761, -0.0478,  0.0866, -0.0437,  0.0125,  0.2150,  0.0669, -0.0109,\n",
       "                       0.0226,  0.1030, -0.0300,  0.1107, -0.1028,  0.1116, -0.2282, -0.0806,\n",
       "                       0.1406,  0.0685,  0.1900, -0.0471,  0.0378, -0.1648,  0.0583,  0.1094,\n",
       "                       0.1448, -0.1852, -0.0412, -0.1075,  0.0478, -0.0428,  0.1756,  0.0694,\n",
       "                       0.2114, -0.0282,  0.1550, -0.1410,  0.0655,  0.0402,  0.0563,  0.1945,\n",
       "                       0.2500, -0.0124,  0.0806, -0.0253,  0.0956, -0.1890, -0.0412, -0.1692,\n",
       "                       0.1026, -0.0056, -0.0480, -0.0088, -0.0490,  0.1571, -0.0032,  0.1266,\n",
       "                       0.0780,  0.2115,  0.0938, -0.0738, -0.0542, -0.1004,  0.1998,  0.0929],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0265, -0.0763,  0.0263,  ...,  0.0455, -0.0270,  0.0171],\n",
       "                      [ 0.1296,  0.1496, -0.1877,  ...,  0.0686,  0.1482, -0.2019],\n",
       "                      [ 0.0088, -0.0166,  0.0283,  ..., -0.0618, -0.0717, -0.1287],\n",
       "                      ...,\n",
       "                      [-0.0592,  0.1247, -0.0610,  ...,  0.0416, -0.2195,  0.0218],\n",
       "                      [ 0.0471, -0.0845,  0.0894,  ..., -0.0793,  0.0703,  0.0760],\n",
       "                      [ 0.1193, -0.2479, -0.2463,  ...,  0.1484, -0.2215, -0.1725]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 7.6308e-02, -7.9328e-02,  1.5513e-02,  1.4483e-02, -9.6417e-02,\n",
       "                       3.0884e-01,  1.3535e-01, -2.4853e-04, -6.5687e-02, -2.3547e-01],\n",
       "                     device='mps:0'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 저장된 가중치 파일(pth) 불러오기\n",
    "new_model = NeuralNetwork().to(device)\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 10.5%, Avg Loss : 2.320468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련되지 않은 모델로 예측\n",
    "test_loop(test_dataloader, new_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/f86w2r851zn7yj1w80yw932h0000gn/T/ipykernel_519/3047005004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_model.load_state_dict(torch.load(\"../../data/saved_models/model_weights.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 85.0%, Avg Loss : 0.423008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이전에 훈련된 가중치를 불러와 새로운 모델에 load\n",
    "new_model.load_state_dict(torch.load(\"../../data/saved_models/model_weights.pth\"))\n",
    "test_loop(test_dataloader, new_model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련된 모델 자체를 저장 / 불러오기\n",
    "\n",
    "* 모델의 구조를 몰라도 사용 가능\n",
    "* 용량은 많이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../../data/saved_models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/f86w2r851zn7yj1w80yw932h0000gn/T/ipykernel_519/2093080944.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_model = torch.load(\"../../data/saved_models/model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 85.0%, Avg Loss : 0.423008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_model = torch.load(\"../../data/saved_models/model.pth\")\n",
    "test_loop(test_dataloader, new_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
