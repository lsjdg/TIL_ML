{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"../../data\",  # 데이터를 저장할 root 디렉토리\n",
    "    train=True,  # 훈련용 데이터 설정\n",
    "    download=True,  # 다운로드\n",
    "    transform=ToTensor(),  # 이미지 변환. 여기서는 TorchTesnor로 변환시킵니다.\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"../../data\", train=False, download=True, transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 가속을 사용합니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"MPS 가속을 사용합니다.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS 가속을 사용할 수 없습니다. CPU를 사용합니다.\")\n",
    "\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # subclass 인 neural network 의 생성자\n",
    "        # 상위 클래스인 nn.Module 의 생성에 대한 책임을 져야 한다. => super 를 통해 상위 클래스의 생성자 호출\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # 생성자에는 항상 레이어의 구성을 정의\n",
    "        self.flatten = nn.Flatten()  # 입력되는 3차원 데이터를 평탄화\n",
    "\n",
    "        # nn.Sequential 을 이용해 연속되는 레이어의 구조를 구성\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # 1층의 구조\n",
    "            nn.Linear(28 * 28, 128),  # 28*28 을 받는 128개의 뉴런\n",
    "            nn.ReLU(),\n",
    "            # 2층(출력층) 구조\n",
    "            nn.Linear(128, 10),\n",
    "            # Softmax 는 나중에 실제 모델에서 확률값을 기반한 훈련 시 사용해도 됨\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward 에는 gray scale 입력 데이터 x 가 들어옴 => x 의 shape 은 (N, 28, 28)\n",
    "        x = self.flatten(x)\n",
    "        y = self.linear_relu_stack(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련\n",
    "\n",
    "파이토치의 모델 훈련을 위해서는 손실함수, 최적화 함수를 등록해야 합니다. 특히 최적화 함수를 사용하기 위해서는 `model.parameters()` 메소드를 이용해 최적화 대상 파라미터를 지정해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer : 경사하강법을 수행하기 위한 함수\n",
    "# 경사하강법은 parameters(W, b) 에 수행됨\n",
    "# 모델에서 파라미터를 꺼내 최적화 함수에 등록\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 과정 (define training loop)\n",
    "1. dataloader 에서 데이터를 꺼낸다\n",
    "2. 데이터를 모델에 통과시킨다 (순전파를 통한 prediction)\n",
    "3. 얻어낸 예측값을 이용해 loss 계산\n",
    "4. 역전파를 통해 미분값 계산\n",
    "5. 얻어낸 미분 값으로 경사하강법 수행 (optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # 모델을 훈련 모드로 설정\n",
    "    model.train()\n",
    "\n",
    "    # 데이터 꺼내기. for문을 사용하면 자동으로 next(iter(dataloader))가 실행됨\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 현재 dataloader에 있는 데이터는 cpu에 존재 => gpu로 옮겨준다\n",
    "        # 모델이 위치한 곳과 데이터가 위치한 곳을 동일하게 맞춰야 함\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 순전파 수행\n",
    "        pred = model(X)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = loss_fn(pred, y)  # 자동으로 Softmax 적용\n",
    "\n",
    "        # 역전파 수행\n",
    "        optimizer.zero_grad()  # 기존에 남아있던 기울기를 제거 (이전 batch 의 기울기가 남아있으면 정확한 값 얻기 힘들다)\n",
    "        loss.backward()  # 역전파, loss 가 leaf\n",
    "        optimizer.step()  # 구한 미분값을 토대로 최적화 수행 (경사하강법)\n",
    "\n",
    "        # 배치가 100번 돌 때마다 화면에 출력\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Train Loss : {loss:>7f} [ {current:>5d} / {size:>5d} ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론을 위한 test loop 정의\n",
    "\n",
    "1. test dataloader 에서 데이터 꺼내기\n",
    "2. 순전파를 통한 예측값 얻기\n",
    "3. metric 계산 (loss, accuracy)\n",
    "    * batch 별 평균 성능 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):  # test 에서는 optimizer 필요없다\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # 평가모드(추론모드) 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 추론 과정은 기울기를 구할 필요가 없다\n",
    "    with torch.no_grad():  # 기울기를 구하지 않는 모드로 전환\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # 맞힌 갯수 총합\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(\n",
    "        f\"Test Error : \\n Accuracy : {(100*correct):>0.1f}%, Avg Loss : {test_loss:>8f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1\n",
      "..................\n",
      "Train Loss : 2.322644 [     0 / 60000 ]\n",
      "Train Loss : 1.564183 [  6400 / 60000 ]\n",
      "Train Loss : 1.128416 [ 12800 / 60000 ]\n",
      "Train Loss : 1.021525 [ 19200 / 60000 ]\n",
      "Train Loss : 0.770508 [ 25600 / 60000 ]\n",
      "Train Loss : 0.672511 [ 32000 / 60000 ]\n",
      "Train Loss : 0.644357 [ 38400 / 60000 ]\n",
      "Train Loss : 0.635989 [ 44800 / 60000 ]\n",
      "Train Loss : 0.587226 [ 51200 / 60000 ]\n",
      "Train Loss : 0.480518 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 78.5%, Avg Loss : 0.630207\n",
      "\n",
      "Epochs 2\n",
      "..................\n",
      "Train Loss : 0.589777 [     0 / 60000 ]\n",
      "Train Loss : 0.628243 [  6400 / 60000 ]\n",
      "Train Loss : 0.516036 [ 12800 / 60000 ]\n",
      "Train Loss : 0.682999 [ 19200 / 60000 ]\n",
      "Train Loss : 0.572935 [ 25600 / 60000 ]\n",
      "Train Loss : 0.421131 [ 32000 / 60000 ]\n",
      "Train Loss : 0.482102 [ 38400 / 60000 ]\n",
      "Train Loss : 0.570246 [ 44800 / 60000 ]\n",
      "Train Loss : 0.546395 [ 51200 / 60000 ]\n",
      "Train Loss : 0.546354 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 81.6%, Avg Loss : 0.538733\n",
      "\n",
      "Epochs 3\n",
      "..................\n",
      "Train Loss : 0.436993 [     0 / 60000 ]\n",
      "Train Loss : 0.464249 [  6400 / 60000 ]\n",
      "Train Loss : 0.459811 [ 12800 / 60000 ]\n",
      "Train Loss : 0.553196 [ 19200 / 60000 ]\n",
      "Train Loss : 0.420749 [ 25600 / 60000 ]\n",
      "Train Loss : 0.385661 [ 32000 / 60000 ]\n",
      "Train Loss : 0.532240 [ 38400 / 60000 ]\n",
      "Train Loss : 0.512001 [ 44800 / 60000 ]\n",
      "Train Loss : 0.426991 [ 51200 / 60000 ]\n",
      "Train Loss : 0.393352 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.0%, Avg Loss : 0.496582\n",
      "\n",
      "Epochs 4\n",
      "..................\n",
      "Train Loss : 0.397811 [     0 / 60000 ]\n",
      "Train Loss : 0.351192 [  6400 / 60000 ]\n",
      "Train Loss : 0.417951 [ 12800 / 60000 ]\n",
      "Train Loss : 0.514547 [ 19200 / 60000 ]\n",
      "Train Loss : 0.352419 [ 25600 / 60000 ]\n",
      "Train Loss : 0.524034 [ 32000 / 60000 ]\n",
      "Train Loss : 0.596367 [ 38400 / 60000 ]\n",
      "Train Loss : 0.359611 [ 44800 / 60000 ]\n",
      "Train Loss : 0.296683 [ 51200 / 60000 ]\n",
      "Train Loss : 0.682919 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.2%, Avg Loss : 0.476999\n",
      "\n",
      "Epochs 5\n",
      "..................\n",
      "Train Loss : 0.430919 [     0 / 60000 ]\n",
      "Train Loss : 0.459141 [  6400 / 60000 ]\n",
      "Train Loss : 0.541201 [ 12800 / 60000 ]\n",
      "Train Loss : 0.270785 [ 19200 / 60000 ]\n",
      "Train Loss : 0.329016 [ 25600 / 60000 ]\n",
      "Train Loss : 0.353999 [ 32000 / 60000 ]\n",
      "Train Loss : 0.435707 [ 38400 / 60000 ]\n",
      "Train Loss : 0.352455 [ 44800 / 60000 ]\n",
      "Train Loss : 0.319293 [ 51200 / 60000 ]\n",
      "Train Loss : 0.565377 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.7%, Avg Loss : 0.459831\n",
      "\n",
      "Epochs 6\n",
      "..................\n",
      "Train Loss : 0.485514 [     0 / 60000 ]\n",
      "Train Loss : 0.336467 [  6400 / 60000 ]\n",
      "Train Loss : 0.389297 [ 12800 / 60000 ]\n",
      "Train Loss : 0.444385 [ 19200 / 60000 ]\n",
      "Train Loss : 0.319682 [ 25600 / 60000 ]\n",
      "Train Loss : 0.439162 [ 32000 / 60000 ]\n",
      "Train Loss : 0.309463 [ 38400 / 60000 ]\n",
      "Train Loss : 0.564958 [ 44800 / 60000 ]\n",
      "Train Loss : 0.316228 [ 51200 / 60000 ]\n",
      "Train Loss : 0.470361 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 83.8%, Avg Loss : 0.456105\n",
      "\n",
      "Epochs 7\n",
      "..................\n",
      "Train Loss : 0.380549 [     0 / 60000 ]\n",
      "Train Loss : 0.422735 [  6400 / 60000 ]\n",
      "Train Loss : 0.587097 [ 12800 / 60000 ]\n",
      "Train Loss : 0.377634 [ 19200 / 60000 ]\n",
      "Train Loss : 0.254230 [ 25600 / 60000 ]\n",
      "Train Loss : 0.420574 [ 32000 / 60000 ]\n",
      "Train Loss : 0.410141 [ 38400 / 60000 ]\n",
      "Train Loss : 0.487576 [ 44800 / 60000 ]\n",
      "Train Loss : 0.425355 [ 51200 / 60000 ]\n",
      "Train Loss : 0.309993 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 84.6%, Avg Loss : 0.436143\n",
      "\n",
      "Epochs 8\n",
      "..................\n",
      "Train Loss : 0.289455 [     0 / 60000 ]\n",
      "Train Loss : 0.492617 [  6400 / 60000 ]\n",
      "Train Loss : 0.340204 [ 12800 / 60000 ]\n",
      "Train Loss : 0.407094 [ 19200 / 60000 ]\n",
      "Train Loss : 0.431901 [ 25600 / 60000 ]\n",
      "Train Loss : 0.147657 [ 32000 / 60000 ]\n",
      "Train Loss : 0.323784 [ 38400 / 60000 ]\n",
      "Train Loss : 0.262450 [ 44800 / 60000 ]\n",
      "Train Loss : 0.295730 [ 51200 / 60000 ]\n",
      "Train Loss : 0.319123 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 84.7%, Avg Loss : 0.429779\n",
      "\n",
      "Epochs 9\n",
      "..................\n",
      "Train Loss : 0.205908 [     0 / 60000 ]\n",
      "Train Loss : 0.403312 [  6400 / 60000 ]\n",
      "Train Loss : 0.251180 [ 12800 / 60000 ]\n",
      "Train Loss : 0.293389 [ 19200 / 60000 ]\n",
      "Train Loss : 0.572662 [ 25600 / 60000 ]\n",
      "Train Loss : 0.318975 [ 32000 / 60000 ]\n",
      "Train Loss : 0.540707 [ 38400 / 60000 ]\n",
      "Train Loss : 0.320238 [ 44800 / 60000 ]\n",
      "Train Loss : 0.247194 [ 51200 / 60000 ]\n",
      "Train Loss : 0.339186 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 85.0%, Avg Loss : 0.422200\n",
      "\n",
      "Epochs 10\n",
      "..................\n",
      "Train Loss : 0.572541 [     0 / 60000 ]\n",
      "Train Loss : 0.381614 [  6400 / 60000 ]\n",
      "Train Loss : 0.503358 [ 12800 / 60000 ]\n",
      "Train Loss : 0.419890 [ 19200 / 60000 ]\n",
      "Train Loss : 0.516720 [ 25600 / 60000 ]\n",
      "Train Loss : 0.342155 [ 32000 / 60000 ]\n",
      "Train Loss : 0.291165 [ 38400 / 60000 ]\n",
      "Train Loss : 0.313345 [ 44800 / 60000 ]\n",
      "Train Loss : 0.255781 [ 51200 / 60000 ]\n",
      "Train Loss : 0.359113 [ 57600 / 60000 ]\n",
      "Test Error : \n",
      " Accuracy : 85.2%, Avg Loss : 0.419098\n",
      "\n",
      "Experiment Successful\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epochs {i+1}\\n..................\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Experiment Successful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
