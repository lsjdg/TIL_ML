{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        # 순전파 때 x 값이 음수였던 위치를 mask => 역전파 때 0으로 내보낼 수 있도록\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0\n",
    "        out = x.copy()\n",
    "\n",
    "        out[self.mask] = 0  # numpy 에서 주로 사용하는 boolean mask indexing\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        d_out : 뒤에서부터 전달된 미분값이 들어있는 배열이 들어온다\n",
    "        \"\"\"\n",
    "        d_out[self.mask] = 0  # 순전파 때 음수였던 부분을 0으로 만들어준다\n",
    "\n",
    "        d_x = d_out\n",
    "        return d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "\n",
    "relu = ReLU()\n",
    "relu.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.],\n",
       "       [ 0.,  3.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_out = np.array([[-1.0, 0.5], [-2.0, 3.0]])\n",
    "\n",
    "relu.backward(d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\sigma$(시그모이드) 구현\n",
    "$$\n",
    "\\sigma(x) = \\frac{\\mathrm{1} }{\\mathrm{1} + exp(-x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\sigma(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y' = \\sigma(x)(1-\\sigma(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "y' = y(1-y)\n",
    "$$\n",
    "\n",
    "* 시그모이드 레이어에서 순전파 때 기억하고 있어야 할 값 : $y$값만 알고 있으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        # 순전파 때 구한 y로 역전파 수행 시 y(1-y) 계산\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out  # 역전파에 사용하기 위해 저장\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_x = d_out * self.out * (1 - self.out)\n",
    "        return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear(Dense, FCL) 계층\n",
    "* `forward`\n",
    "  1. 입력값 `x`와 가중치 `W`의 내적 + `b` (`WX+b`)\n",
    "  2. 평탄화\n",
    "    * 1차원 배열로 데이터가 들어왔을 때 대응\n",
    "    * 다차원 배열( 텐서 )에 대한 대응\n",
    "    * **원본 데이터의 형상을 저장**\n",
    "* `backward`\n",
    "  1. 미분값(`dout`)과 가중치의 전치행렬(`W.T`) 내적 ( 입력값에 대한 미분값 )\n",
    "  2. 입력값의 전치행렬(`X.T`)과 미분값(`dout`) 내적 ( 가중치에 대한 미분값 )\n",
    "  3. 배치를 축으로 편향 합 구하기 (`axis=0`)\n",
    "  4. 입력값의 미분값(`dx`)의 형상을 원본 `x`의 형상으로 다시 바꿔주기\n",
    "\n",
    "`forward`에 의해서 형상(shape) 변환이 일어나기 때문에 `backward`할 때 원본 모양으로 되돌려 준다.\n",
    "* `(100, 28, 28)`이 입력으로 들어오면, `forward`에서 평탄화에 의해 `(100, 784)`가 된다.\n",
    "* 텐서에도 모두 내적을 수행할 수 있도록 원본 형상인 `(100, 28, 28)`저장을 해 놨다가 `backward`할 때 저장했던 원본 모양으로 **미분값 배열**의 형상을 되돌려 준다.\n",
    "\n",
    "**저장해야 할 값**\n",
    "1. 원본 `x`의 형상(shape)\n",
    "2. 원본 `x` 데이터\n",
    "3. 가중치, 편향\n",
    "4. `dW`, `db`\n",
    "  * 미분값을 알고 있어야 나중에 최적화( Optimization )를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, W, b):\n",
    "        # 가중치, 편향, 입력값\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "\n",
    "        # x 의 shape 저장 : 지금은 평탄화 된 데이터를 사용하기 때문에 필수는 아니지만, 나중에 CNN 등 데이터가 1차원이 아닐 때 필요\n",
    "        self.original_x_shape = None\n",
    "\n",
    "        # W 와 b의 기울기 배열 저장 => Optimization 을 위해 각 매개변수의 미분값이 필요\n",
    "        self.d_W = None\n",
    "        self.d_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응을 위해 입렦밧 x의 shape 저장\n",
    "        # ex) x 가 (3, 2, 2) => 선형연산을 위해서는 (3, 2*2)로 평탄화 필요 => 역전파시 다시 (3, 2, 2)로 바꿔줘야 함\n",
    "        self.original_x_shape = x.shape\n",
    "\n",
    "        # 평탄화\n",
    "        DATA_SIZE = x.shape[0]  # batch size\n",
    "        x = x.reshape(DATA_SIZE, -1)\n",
    "\n",
    "        self.x = x\n",
    "        out = self.x @ self.W + self.b  # (N, M) @ (M, K) + (K, ) => (N, K)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):  # d_out 의 shape : (N, K)\n",
    "        # 입력값에 내보내는 미분값 계산\n",
    "        d_x = d_out @ self.W.T  # (N, K) @ (K, M) => (N, M)\n",
    "\n",
    "        self.d_W = self.x.T @ d_out  # (M, N) @ (N, K) => (M, K)\n",
    "        # (K, ) 였던 bias 가 순전파를 거치면서 batch size 만큼 증폭됨 (batch 마다 더해져서)\n",
    "        # 역전파에서 다시 바꿔줘야 한다\n",
    "        self.d_b = np.sum(d_out, axis=0)  # (N, K) => (K, )\n",
    "\n",
    "        return d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실값 (시각화 할 때 사용할 예정..)\n",
    "        self.y = None  # 예측값 ( 역전파 때 사용 )\n",
    "        self.t = None  # 정답 레이블 ( 역전파 때 사용 )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, d_out=1):  # dout이 1인 이유 : d돈통 / d포스기 개념\n",
    "        # 배치 고려하기\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        # t가 원-핫 인코딩이 되어있는지, 안되어 있는지 고려\n",
    "        if (\n",
    "            self.t.size == self.y.size\n",
    "        ):  # 출력층의 원소 개수를 비교하는 것은 원-핫 인코딩이 되어있는 t\n",
    "            # 항상 y는 softmax의 결과물 (N, OUTPUT_SIZE)\n",
    "            # t가 OHE가 되어 있으면 (N, OUTPUT_SIZE)\n",
    "            # t가 OHE가 안되어 있으면 (N, )\n",
    "            d_x = (self.y - self.t) / batch_size\n",
    "        else:  # t가 OHE가 안되어 있는 경우\n",
    "            d_x = self.y.copy()\n",
    "\n",
    "            # 원-핫 인코딩이 되어있지 않은 t는 정답 레이블의 인덱스로 생각할 수 있다.\n",
    "            # y = [0.2, 0.1, 0.7], t = 2\n",
    "            # dx[np.arange(batch_size), self.t] -> dx[0, 2] -> 0.7 -> 0.7 - 1 를 구하겠다는 이야기 이다. -> -0.3의 오차가 있다.\n",
    "            d_x[np.arange(batch_size), self.t] -= 1\n",
    "            d_x = d_x / batch_size\n",
    "\n",
    "        return d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderedDict : 순서가 있는 딕셔너리\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# 2층 신경망 구현\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 매개변수 초기화 (모델 파라미터 -> 미분이 가능해야 한다)\n",
    "        self.params = {}\n",
    "\n",
    "        # 1층 은닉층을 위한 파라미터\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "\n",
    "        # 2층 출력층을 위한 매개변수\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "\n",
    "        # 레이어 정의\n",
    "        # 추가되는 레이어의 순서가 바뀌면 안되기 때문에 OrderedDict 사용\n",
    "        self.layers = OrderedDict()\n",
    "\n",
    "        self.layers[\"linear_1\"] = Linear(self.params[\"W1\"], self.params[\"b1\"])\n",
    "        self.layers[\"relu\"] = ReLU()\n",
    "\n",
    "        self.layers[\"linear_2\"] = Linear(self.params[\"W2\"], self.params[\"b2\"])\n",
    "\n",
    "        # 마지막 층은 항상 SoftmaxWithLoss\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 각 층의 forward 함수를 순차적으로 실행\n",
    "        for layer in self.layers.values():  # 레이어를 순서대로 가져오기\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        # 예측을 하고, 정답이랑 얼마나 틀렸는지를 계산\n",
    "        # 단, t가 원-핫 인코딩이 되어있는지, 안되어있는지가 중요!\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(\n",
    "            y, axis=1\n",
    "        )  # 제일 큰 값 하나만 뽑자.( 제일 확률이 높은 곳에 위치한 인덱스를 갖는다. )\n",
    "\n",
    "        # 원-핫 인코딩 처리\n",
    "        # t.ndim != 1 --> t가 원핫 인코딩이 되어있는 상태라면\n",
    "        # t에서 제일 높은 인덱스를 찾겠다.\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    # 역전파를 활용한 각 매개변수의 기울기 배열 구하기\n",
    "    def backward_propagation(self, x, t):\n",
    "        # 순전파를 통해 오차를 구한다\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        d_out = 1  # dL/dL = 1\n",
    "\n",
    "        # SoftmaxWithLoss의 오차에 대한 미분값 구하기\n",
    "        d_out = self.last_layer.backward(d_out)  # 순수한 오차 y-t\n",
    "\n",
    "        # 은닉충에 해당하는 레이어만 받아와서 뒤집기\n",
    "        hidden_layers = list(self.layers.values())  # 순서대로 되어있는 레이어\n",
    "        hidden_layers.reverse()\n",
    "\n",
    "        # 뒤집어진 레이어를 하나씩 꺼내기\n",
    "        for layer in hidden_layers:\n",
    "            # 미분값을 집어넣으며 역전파\n",
    "            d_out = layer.backward(d_out)\n",
    "\n",
    "        # 역전파가 끝났으면 각 레이어마다 기울기(d_W, d_b)가 들어있다\n",
    "        grads = {}\n",
    "\n",
    "        grads[\"W1\"], grads[\"b1\"] = (\n",
    "            self.layers[\"linear_1\"].d_W,\n",
    "            self.layers[\"linear_1\"].d_b,\n",
    "        )\n",
    "\n",
    "        grads[\"W2\"], grads[\"b2\"] = (\n",
    "            self.layers[\"linear_2\"].d_W,\n",
    "            self.layers[\"linear_2\"].d_b,\n",
    "        )\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets  # type: ignore\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "mnist = datasets.mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "y_train_dummy = OneHotEncoder().fit_transform(y_train.reshape(-1, 1))\n",
    "y_train_dummy = y_train_dummy.toarray()\n",
    "\n",
    "y_test_dummy = OneHotEncoder().fit_transform(y_test.reshape(-1, 1))\n",
    "y_test_dummy = y_test_dummy.toarray()\n",
    "\n",
    "# feature 전처리\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_train = (\n",
    "    X_train / 255.0\n",
    ")  # 이미지 정규화 기법. 255.0 으로 나눠주면 모든 픽셀 데이터가 0 ~ 1사이의 값을 갖게 되고, 훈련이 쉽게 된다.\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(input_size=28 * 28, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "iter_nums = 10000  # 학습 반복 횟수\n",
    "learning_rate = 0.1\n",
    "batch_size = 100\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "# 훈련 과정을 1에폭마다 기록 (시각화 하기 위해)\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "train_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ==> Train Accuracy : 0.116483 / Test Accuracy : 0.117200 / Train Loss : 2.300136 / Test Loss : 2.302532 \n",
      "Epoch 1 ==> Train Accuracy : 0.902117 / Test Accuracy : 0.906300 / Train Loss : 0.384542 / Test Loss : 0.313861 \n",
      "Epoch 2 ==> Train Accuracy : 0.928083 / Test Accuracy : 0.927300 / Train Loss : 0.224303 / Test Loss : 0.244362 \n",
      "Epoch 3 ==> Train Accuracy : 0.941517 / Test Accuracy : 0.939900 / Train Loss : 0.129607 / Test Loss : 0.207493 \n",
      "Epoch 4 ==> Train Accuracy : 0.948483 / Test Accuracy : 0.945700 / Train Loss : 0.115185 / Test Loss : 0.182400 \n",
      "Epoch 5 ==> Train Accuracy : 0.955400 / Test Accuracy : 0.951400 / Train Loss : 0.074273 / Test Loss : 0.162393 \n",
      "Epoch 6 ==> Train Accuracy : 0.961617 / Test Accuracy : 0.957600 / Train Loss : 0.073201 / Test Loss : 0.141216 \n",
      "Epoch 7 ==> Train Accuracy : 0.964700 / Test Accuracy : 0.960200 / Train Loss : 0.078577 / Test Loss : 0.132643 \n",
      "Epoch 8 ==> Train Accuracy : 0.967733 / Test Accuracy : 0.962100 / Train Loss : 0.168717 / Test Loss : 0.125961 \n",
      "Epoch 9 ==> Train Accuracy : 0.971683 / Test Accuracy : 0.965400 / Train Loss : 0.079926 / Test Loss : 0.114706 \n",
      "Epoch 10 ==> Train Accuracy : 0.972933 / Test Accuracy : 0.966700 / Train Loss : 0.113468 / Test Loss : 0.110970 \n",
      "Epoch 11 ==> Train Accuracy : 0.974367 / Test Accuracy : 0.968800 / Train Loss : 0.045148 / Test Loss : 0.104049 \n",
      "Epoch 12 ==> Train Accuracy : 0.978233 / Test Accuracy : 0.970800 / Train Loss : 0.034492 / Test Loss : 0.098108 \n",
      "Epoch 13 ==> Train Accuracy : 0.979483 / Test Accuracy : 0.970400 / Train Loss : 0.104259 / Test Loss : 0.096881 \n",
      "Epoch 14 ==> Train Accuracy : 0.980550 / Test Accuracy : 0.973400 / Train Loss : 0.027217 / Test Loss : 0.091688 \n",
      "Epoch 15 ==> Train Accuracy : 0.982017 / Test Accuracy : 0.972900 / Train Loss : 0.026042 / Test Loss : 0.088819 \n",
      "Epoch 16 ==> Train Accuracy : 0.983517 / Test Accuracy : 0.974000 / Train Loss : 0.083292 / Test Loss : 0.085660 \n"
     ]
    }
   ],
   "source": [
    "# 1 에폭에 필요한 훈련 횟수\n",
    "iter_per_epoch = int(max(train_size / batch_size, 1))\n",
    "\n",
    "for i in range(iter_nums):\n",
    "    # 미니 배치 생성\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    # 훈련 (경사하강법에 의한 가중치, 편향 갱신)\n",
    "    grads = model.backward_propagation(X_batch, y_batch)  # 기울기 얻기\n",
    "\n",
    "    # optimization\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "        model.params[key] -= learning_rate * grads[key]\n",
    "\n",
    "    # 1에폭 마다 정확도, loss 확인\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_loss = model.loss(X_batch, y_batch)\n",
    "        test_loss = model.loss(X_test, y_test)\n",
    "\n",
    "        train_acc = model.accuracy(X_train, y_train)\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\n",
    "            \"Epoch {} ==> Train Accuracy : {:.6f} / Test Accuracy : {:.6f} / Train Loss : {:.6f} / Test Loss : {:.6f} \".format(\n",
    "                int(i / iter_per_epoch), train_acc, test_acc, train_loss, test_loss\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
